{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true = pd.read_csv(\"True.csv\")\n",
    "true[\"label\"] = 1\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "fake[\"label\"] = 0\n",
    "main = [true, fake]\n",
    "main = pd.concat(main, ignore_index=True)\n",
    "main = main.drop([\"date\", \"subject\"], axis=1)\n",
    "main[\"length\"] = main['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Input data has {} rows and {} columns\".format(len(main), len(main.columns)))\n",
    "print(\"Out of {} rows, {} are spam, {} are ham\".format(len(main), len(main[main['label'] == 0]),\n",
    "                                                       len(main[main['label'] == 1])))\n",
    "print(\"Number of null in label: {}\".format(main['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(main['text'].isnull().sum()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "# ps = nltk.PorterStemmer()\n",
    "stoplist = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stoplist]\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(main['text'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names())\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()\n",
    "X_tfidf_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualizing the true data\n",
    "true.subject.value_counts().plot.bar()\n",
    "plt.title(\"True news articles for different subjects\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "fake.subject.value_counts().plot.bar()\n",
    "plt.title(\"Fake news articles for different subjects\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "bins = np.linspace(0, 10000, 40)\n",
    "plt.hist(main[main['label'] == 0]['length'], bins, alpha=0.5, label='fake', density=True)\n",
    "plt.hist(main[main['label'] == 1]['length'], bins, alpha=0.5, label='true', density=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}